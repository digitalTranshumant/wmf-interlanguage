{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section Recommendation / Inter-lingual approach\n",
    "\n",
    "This notebook implements section recommendation based on the Section Aligments. \n",
    "Given an article in language a target language T, though Wikidata retrieves the section of the same in article in other languages, and provides a recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "\n",
    "#Config\n",
    "suportedLangs = ['fr','en','es','ja','ar','ru']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads alignments \n",
    "\n",
    "import os \n",
    "rec = {}\n",
    "for f in os.listdir('recSheetsTSV/'):\n",
    "    lang1 = f[0:2]\n",
    "    lang2 = f[6:8]\n",
    "    rec[lang1] = rec.get(lang1,{})\n",
    "    rec[lang1][lang2] = pd.read_csv('recSheetsTSV/'+f,sep='\\t')\n",
    "    rec[lang1][lang2].set_index('secFrom',inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Section parser\n",
    "sections_RE = re.compile(r'(^|[^=])==([^=\\n\\r]+)==([^=]|$)')\n",
    "def extract_sections(text):\n",
    "    for m in sections_RE.finditer(text):\n",
    "        yield m.group(2).strip()\n",
    "\n",
    "#Get articles\n",
    "def getContent(title,lang):\n",
    "    url = \"https://%s.wikipedia.org/w/api.php?action=query&prop=revisions&rvprop=content&format=json&formatversion=2&titles=%s\" % (lang,title)\n",
    "    response = requests.get(url)\n",
    "    content = response.json()['query']['pages'][0]['revisions'][0]['content']\n",
    "    return content\n",
    "\n",
    "def getPages(title,lang,target=suportedLangs):\n",
    "    \"\"\"\n",
    "    title: page title in target language\n",
    "    lang: target language\n",
    "    target: List of Pages \n",
    "    returns a dictionary 'x_wiki':x_title\n",
    "    \"\"\"\n",
    "    response= requests.get(\"https://www.wikidata.org/w/api.php?action=wbgetentities&sites=%swiki&titles=%s&props=sitelinks&format=json\" % (lang,title))\n",
    "    output ={}\n",
    "    assert list(response.json()['entities'].values())[0]['sitelinks'], \"Oh no! This assertion failed!\"\n",
    "    links = list(response.json()['entities'].values())[0]['sitelinks']\n",
    "    for t in target:\n",
    "        if t+'wiki' in links:\n",
    "            output[t] = links[t+'wiki' ]['title']\n",
    "        \n",
    "    return output\n",
    "\n",
    "def getAllLangs(title,lang):\n",
    "    \"\"\"\n",
    "    title: page title in target language\n",
    "    lang: target language\n",
    "    returns a dictionary 'x_wiki':list_of_sections_in_x \n",
    "    \"\"\"\n",
    "    secs = {}\n",
    "    for l,page in getPages(title,lang).items():\n",
    "        print(l,page)\n",
    "        secs[l]  = list(extract_sections(getContent(page,l)))\n",
    "    return secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "#For each section in language X, we have N possible mappings with certain probability. \n",
    "#Ex: 'Vida temprana' in Spanish maps to 'Early Life,p=0.9','Early Years,p=.8','References,p=0.3' in English\n",
    "#Given two languages X and Y, we want to find the most similar clusters considering the mapped sections\n",
    "def getMostSimilarClusters(c1,c2): \n",
    "    \"\"\"\n",
    "    c1: dictionary of sections in language 1 (allready mapped to target language) with a giving probability\n",
    "    c2: dictionary of sections in language 2 (allready mapped to target languages) with a giving probability\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    mostSimilar = []\n",
    "    for pos1,dict_1 in enumerate(c1):\n",
    "        for pos2,dict_2 in enumerate(c2):\n",
    "            dot_product = sum(dict_1[key]*dict_2.get(key, 0) for key in dict_1)\n",
    "            G.add_edge((1,pos1),(2,pos2))\n",
    "            G[(1,pos1)][(2,pos2)]['w'] = dot_product\n",
    "    for a, b, data in sorted(G.edges(data=True), key=lambda x: x[2]['w'],reverse=True):\n",
    "        try:\n",
    "            G.remove_node(a)\n",
    "            G.remove_node(b)   \n",
    "            sortedNodes  = sorted([a,b])\n",
    "            mostSimilar.append((sortedNodes[0][1],sortedNodes[1][1]))\n",
    "        except:\n",
    "            pass #one of the nodes was already paried\n",
    "    return mostSimilar\n",
    "\n",
    "#Given a template dictionary of sections c1\n",
    "#and another dictionary of sections c2\n",
    "#update  probabilities on c1 based on information c2\n",
    "def updateClusterWeights(c1,c2): #c1 is the template to be updated\n",
    "    \"\"\"\n",
    "    c1: dictionary of sections in language 1 (allready mapped to target language) with a giving probability\n",
    "    c2: dictionary of sections in language 2 (allready mapped to target languages) with a giving probability\n",
    "    \"\"\"\n",
    "    for s1,s2 in getMostSimilarClusters(c1,c2):\n",
    "        for s in c1[s1].keys() & c2[s2].keys():\n",
    "            c1[s1][s] = c1[s1][s] + c2[s2][s]\n",
    "    return c1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRecs(title,TargetLang,verbose=False):\n",
    "    '''\n",
    "    title: Article to get recommendations (ex:'Quilombo')\n",
    "    TargetLang: Language of the article (ex:'en')\n",
    "    verbose: return explanations\n",
    "    '''\n",
    "    #load translations\n",
    "    global rec\n",
    "    # get sections in all Languages\n",
    "    secs = getAllLangs(title,TargetLang)\n",
    "    # get a list of the sources languages\n",
    "    sourceLangs = set(secs.keys()) - {TargetLang}\n",
    "    #count amount of sections in the target language\n",
    "    lenTarget = len(secs[TargetLang])\n",
    "    #count the number of sections in each source lang, produce a tuple (SecCount,Lang)\n",
    "    lenSources = [(len(s),l) for l,s in secs.items() if l != TargetLang]\n",
    "    #use the language with more sections as template\n",
    "    templateLang = max(lenSources)[1]\n",
    "    secsMapped = {}\n",
    "    #For all source languages S, take all sections in S and map to the target languge T, with it's probability\n",
    "    for lang in sourceLangs:\n",
    "        df = rec[lang][TargetLang]\n",
    "        secsMapped[lang] = []\n",
    "        for sec in secs[lang]:\n",
    "            tmp = df[df.index ==sec][['langTo','prob']]\n",
    "            secsMapped[lang].append(dict(zip(tmp.langTo,tmp.prob)))\n",
    "    #Use the language with more sections as template\n",
    "    templateRec = secsMapped[templateLang]\n",
    "    #Update the template using the remaining languages\n",
    "    for lang in sourceLangs:\n",
    "        if lang != templateLang:\n",
    "            templateRec = updateClusterWeights(templateRec,secsMapped[lang])\n",
    "    finalRecs = []\n",
    "    for cluster in templateRec:\n",
    "        if cluster: #check cluster is not empty\n",
    "            candidates = [recTuple[0] for recTuple in sorted(cluster.items(),  key=lambda x: x[1],reverse=True)][:3]\n",
    "            if not (set(candidates) & set(secs[TargetLang])):\n",
    "                finalRecs.append(candidates[0])\n",
    "    if verbose:\n",
    "        output = {}\n",
    "        output['context'] = {}\n",
    "        output['Recommendations'] = json.dumps(list(set(finalRecs)))\n",
    "        output['context'] ['CurrentSections'] = secs[TargetLang]\n",
    "        #create a copy of sections\n",
    "        otherLangs = secs.copy()\n",
    "        del(otherLangs[TargetLang])\n",
    "        output['context']['SectionsInOtherLanguages'] = otherLangs\n",
    "    else:\n",
    "        output = json.dumps(list(set(finalRecs)))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar غابرييل غارثيا ماركيث\n",
      "fr Gabriel García Márquez\n",
      "ru Гарсиа Маркес, Габриэль\n",
      "en Gabriel García Márquez\n",
      "ja ガブリエル・ガルシア＝マルケス\n",
      "es Gabriel García Márquez\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[\"Works\"]'"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " getRecs('Gabriel_García_Márquez','en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ru Киломбу\n",
      "en Quilombo\n",
      "fr Quilombo (esclavage)\n",
      "es Quilombo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[\"Economy\", \"Infrastructure\", \"Organization\"]'"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " getRecs('Quilombo','en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar بديهية (فلسفة)\n",
      "fr Axiome\n",
      "ru Аксиома\n",
      "en Axiom\n",
      "ja 公理\n",
      "es Axioma\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Recommendations': '[\"Historia\"]',\n",
       " 'context': {'CurrentSections': ['Etimología',\n",
       "   'Legado helénico',\n",
       "   'Lógica',\n",
       "   'Matemáticas',\n",
       "   'Limitaciones de los sistemas axiomáticos',\n",
       "   'Véase también',\n",
       "   'Referencias',\n",
       "   'Bibliografía',\n",
       "   'Enlaces externos'],\n",
       "  'SectionsInOtherLanguages': {'ar': ['بديهيات', 'انظر أيضا', 'مراجع'],\n",
       "   'en': ['Etymology',\n",
       "    'Historical development',\n",
       "    'Mathematical logic',\n",
       "    'See also',\n",
       "    'References',\n",
       "    'Further reading',\n",
       "    'External links'],\n",
       "   'fr': ['Histoire', 'Description', 'Références', 'Voir aussi'],\n",
       "   'ja': ['公理の例', '公理の必要性', '歴史', '公理の形式性', '公理の直観的・歴史的な妥当性', '脚注', '関連項目'],\n",
       "   'ru': ['Назначение',\n",
       "    'История',\n",
       "    'Примеры',\n",
       "    'См. также',\n",
       "    'Литература',\n",
       "    'Ссылки',\n",
       "    'Примечания']}}}"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getRecs('Axioma','es',verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
