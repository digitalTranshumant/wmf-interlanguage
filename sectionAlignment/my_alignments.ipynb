{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So, show me how to align two vector spaces for myself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No problem. We're going to run through the example given in the README again, and show you how to learn your own transformation to align the French vector space to the Russian vector space.\n",
    "\n",
    "First, let's define a few simple functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from fasttext import FastVector\n",
    "\n",
    "# from https://stackoverflow.com/questions/21030391/how-to-normalize-array-numpy\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2==0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
    "    \"\"\"\n",
    "    Source and target dictionaries are the FastVector objects of\n",
    "    source/target languages. bilingual_dictionary is a list of \n",
    "    translation pair tuples [(source_word, target_word), ...].\n",
    "    \"\"\"\n",
    "    source_matrix = []\n",
    "    target_matrix = []\n",
    "\n",
    "    for (source, target) in bilingual_dictionary:\n",
    "        try:\n",
    "            source = source.lower().split()\n",
    "            sourceVector = np.zeros(300) + sum([source_dictionary[word] for word in source  if word in source_dictionary])/len(source)\n",
    "            target = target.lower().split()\n",
    "            targetVector = np.zeros(300) + sum([target_dictionary[word] for word in target  if word in target_dictionary])/len(target)\n",
    "            if (sourceVector.all() !=0) and (targetVector.all() != 0):\n",
    "                    source_matrix.append(sourceVector)\n",
    "                    target_matrix.append(targetVector)\n",
    "        except:\n",
    "            pass\n",
    "    # return training matrices\n",
    "    return np.array(source_matrix), np.array(target_matrix)\n",
    "\n",
    "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
    "    \"\"\"\n",
    "    Source and target matrices are numpy arrays, shape\n",
    "    (dictionary_length, embedding_dimension). These contain paired\n",
    "    word vectors from the bilingual dictionary.\n",
    "    \"\"\"\n",
    "    # optionally normalize the training vectors\n",
    "    if normalize_vectors:\n",
    "        source_matrix = normalized(source_matrix)\n",
    "        target_matrix = normalized(target_matrix)\n",
    "\n",
    "    # perform the SVD\n",
    "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "\n",
    "    # return orthogonal transformation which aligns source language to the target\n",
    "    return np.matmul(U, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# get wikidata data\n",
    "df = pd.read_csv('../../gap/wikidataSixLang.csv.gz',sep='\\t',index_col=0,header=None).rename(columns={0:'q',1:'wiki',2:'page'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "reading word vectors from vectors/wiki.en.vec\n",
      "reading word vectors from vectors/wiki.ja.vec\n",
      "== ja\n",
      "reading word vectors from vectors/wiki.ar.vec\n",
      "== ar\n",
      "reading word vectors from vectors/wiki.es.vec\n",
      "== es\n",
      "reading word vectors from vectors/wiki.fr.vec\n",
      "== fr\n",
      "reading word vectors from vectors/wiki.ru.vec\n",
      "== ru\n",
      "ru\n",
      "reading word vectors from vectors/wiki.ja.vec\n",
      "== ja\n",
      "reading word vectors from vectors/wiki.ar.vec\n",
      "== ar\n",
      "reading word vectors from vectors/wiki.es.vec\n",
      "== es\n",
      "reading word vectors from vectors/wiki.fr.vec\n",
      "== fr\n",
      "fr\n",
      "reading word vectors from vectors/wiki.ja.vec\n",
      "== ja\n",
      "reading word vectors from vectors/wiki.ar.vec\n",
      "== ar\n",
      "reading word vectors from vectors/wiki.es.vec\n",
      "== es\n",
      "es\n",
      "reading word vectors from vectors/wiki.ja.vec\n",
      "== ja\n",
      "reading word vectors from vectors/wiki.ar.vec\n",
      "== ar\n",
      "ar\n",
      "reading word vectors from vectors/wiki.ja.vec\n",
      "== ja\n",
      "ja\n"
     ]
    }
   ],
   "source": [
    "import glob,os\n",
    "vectors = sorted(glob.glob('vectors/wiki.*.vec'), key=os.path.getsize) #sorted by size to load the largest files just once\n",
    "vectors.remove('vectors/wiki.fa.vec') #not considering farsi for this experiment\n",
    "lang2 = ''\n",
    "while vectors:\n",
    "    lang1 = vectors.pop()\n",
    "    lang1_code = lang1.split('.')[1]\n",
    "    print(lang1_code)\n",
    "    if lang1 == lang2:\n",
    "        lang1_dictionary = lang2_dictionary\n",
    "    else:\n",
    "        lang1_dictionary = FastVector(vector_file=lang1)\n",
    "    for lang2 in vectors:\n",
    "        lang2_dictionary = FastVector(vector_file=lang2)\n",
    "        lang2_code = lang2.split('.')[1]\n",
    "        print('==',lang2_code)\n",
    "        pairs = df[df.wiki == lang1_code].join(df[df.wiki == lang2_code],rsuffix='_lang2',how='inner')\n",
    "        bilingual_dictionary = list(zip(pairs['page'],pairs['page_lang2']))\n",
    "        #common words\n",
    "        lang1_words = set(lang1_dictionary.word2id.keys())\n",
    "        lang2_words = set(lang2_dictionary.word2id.keys())\n",
    "        overlap = list(lang1_words & lang2_words)\n",
    "        bilingual_dictionary.extend([(entry, entry) for entry in overlap])\n",
    "        # form the training matrices\n",
    "        source_matrix, target_matrix = make_training_matrices(lang1_dictionary, lang2_dictionary, bilingual_dictionary)\n",
    "        # learn and apply the transformation\n",
    "        transform = learn_transformation(source_matrix, target_matrix)\n",
    "        with open('my_alingments/apply_in_%s_to_%s.txt' % (lang1_code,lang2_code),'w') as f:\n",
    "            np.savetxt(f, transform)\n",
    "        bilingual_dictionary = [(y,x) for x,y in bilingual_dictionary] #reverse pairs\n",
    "        # form the training matrices\n",
    "        source_matrix, target_matrix = make_training_matrices(lang2_dictionary, lang1_dictionary, bilingual_dictionary)\n",
    "        # learn and apply the transformation\n",
    "        transform = learn_transformation(source_matrix, target_matrix)\n",
    "        with open('my_alingments/apply_in_%s_to_%s.txt' % (lang2_code,lang1_code),'w') as f:\n",
    "            np.savetxt(f, transform)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from vectors/wiki.ru.vec\n"
     ]
    }
   ],
   "source": [
    "lang1_dictionary = FastVector(vector_file='vectors/wiki.ru.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from vectors/wiki.ar.vec\n"
     ]
    }
   ],
   "source": [
    "lang2_dictionary = FastVector(vector_file='vectors/wiki.ar.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006437103624660425"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastVector.cosine_similarity(lang1_dictionary['История'.lower()],lang2_dictionary['التاريخ'.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test the preinstall aligments\n",
    "lang1_dictionary.apply_transform('alignment_matrices/ru.txt')\n",
    "lang2_dictionary.apply_transform('alignment_matrices/ar.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4295707893064699"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastVector.cosine_similarity(lang1_dictionary['История'.lower()],lang2_dictionary['التاريخ'.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from vectors/wiki.ru.vec\n",
      "reading word vectors from vectors/wiki.ar.vec\n"
     ]
    }
   ],
   "source": [
    "lang1_dictionary = FastVector(vector_file='vectors/wiki.ru.vec')\n",
    "lang2_myaligment = FastVector(vector_file='vectors/wiki.ar.vec')\n",
    "lang2_myaligment.apply_transform('my_alingments/apply_in_ar_to_ru.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5110379249206718"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastVector.cosine_similarity(lang1_dictionary['История'.lower()],lang2_myaligment['التاريخ'.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5952403995146678\n",
      "0.5887240865621274\n"
     ]
    }
   ],
   "source": [
    "print(FastVector.cosine_similarity(lang1_dictionary['toponomy'],lang2_dictionary['toponimia']))\n",
    "print(FastVector.cosine_similarity(lang1_dictionary['toponomy'],lang2_myaligment['toponimia']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7600301173896179\n",
      "0.7209900810626596\n"
     ]
    }
   ],
   "source": [
    "print(FastVector.cosine_similarity(lang1_dictionary['history'],lang2_dictionary['historia']))\n",
    "print(FastVector.cosine_similarity(lang1_dictionary['history'],lang2_myaligment['historia']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -put my_alingments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
