{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read, join and clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Dict to transform labels to a continius number   \n",
    "labelmap = {'not related':0,'overlap':.5,'same':1}\n",
    "\n",
    "#Read all spreadsheets, and give standard names to columns (Original names are language specific)\n",
    "allLabels = pd.read_excel('Synonym mapping.xlsx',sheetname=None)\n",
    "for k in allLabels.keys():\n",
    "        allLabels[k].columns = ['sec1','sec2','relation']\n",
    "\n",
    "#Join labels and precomputed features (check Synonym.ipynb)\n",
    "for k in allLabels.keys():\n",
    "    lang = k[0:2]\n",
    "    features = pd.read_excel('%sSynonyms_Stratified.xls' % lang,index=False)\n",
    "    allLabels[k] = pd.merge(allLabels[k], features, how='left', left_on=['sec1','sec2'],right_on=['Sec_A', 'Sec_B'],suffixes=['',''])\n",
    "    allLabels[k] = allLabels[k] [['sec1','sec2','relation', 'coOccurs','editDistance','isSubSet','tfIdfSimilarity','vectorDistance']]\n",
    "    allLabels[k]['lang']  = lang\n",
    "\n",
    "    #allLabels[k]['relation']  = allLabels[k]['relation'].map(labelmap)\n",
    "\n",
    "#Aggregate all data in one dataframe\n",
    "allData = pd.concat(allLabels.values())\n",
    "allData['binaryRelation'] = allData.relation.map({'not related':'not related','overlap':'related','same':'related'})\n",
    "\n",
    "#drop pairs without label\n",
    "allData.dropna(inplace=True)\n",
    "\n",
    "#get all langs\n",
    "langs = list(allData.lang.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    3441\n",
       "True      221\n",
       "dtype: int64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we remove duplicates \n",
    "allData = allData.drop_duplicates() \n",
    "#We have disgrament in:\n",
    "allData.duplicated(subset=['sec1','sec2','lang']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity I keep all labels. For future experiments this should be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Within Languages\n",
    "We train and test on the same language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "import random\n",
    "random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "Accuracy: 73.23%\n",
      "Accuracy: 32.28%\n",
      "F1 Score: 71.38%\n",
      "F1 Score Random: 35.57%\n",
      "[[82  3  4]\n",
      " [10 10  6]\n",
      " [ 8  3  1]]\n",
      "ar\n",
      "Accuracy: 94.86%\n",
      "Accuracy: 37.71%\n",
      "F1 Score: 93.92%\n",
      "F1 Score Random: 51.20%\n",
      "[[159   0]\n",
      " [  9   7]]\n",
      "es\n",
      "Accuracy: 76.62%\n",
      "Accuracy: 28.57%\n",
      "F1 Score: 75.05%\n",
      "F1 Score Random: 32.40%\n",
      "[[46  2  0]\n",
      " [ 5 10  2]\n",
      " [ 0  9  3]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n",
      "Accuracy: 85.53%\n",
      "Accuracy: 31.06%\n",
      "F1 Score: 84.49%\n",
      "F1 Score Random: 40.34%\n",
      "[[182   4   4]\n",
      " [ 14  12   3]\n",
      " [  5   4   7]]\n",
      "ja\n",
      "Accuracy: 75.12%\n",
      "Accuracy: 40.30%\n",
      "F1 Score: 75.79%\n",
      "F1 Score Random: 48.22%\n",
      "[[142  13   6]\n",
      " [ 11   3   8]\n",
      " [  4   8   6]]\n",
      "ru\n",
      "Accuracy: 69.47%\n",
      "Accuracy: 28.77%\n",
      "F1 Score: 68.46%\n",
      "F1 Score Random: 32.61%\n",
      "[[184  15   1]\n",
      " [ 21   5  20]\n",
      " [  4  26   9]]\n"
     ]
    }
   ],
   "source": [
    "test_size =.3\n",
    "for lang in langs:\n",
    "    data = allData[allData.lang == lang]\n",
    "    Y = data['relation']\n",
    "    X = data[['coOccurs','editDistance','isSubSet','tfIdfSimilarity','vectorDistance']]\n",
    "    X2 = poly.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=1)\n",
    "    model = RandomForestClassifier(n_estimators=100,random_state=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    randomClass = [choice(['not related','overlap','same']) for _ in range(0,len(y_pred))]\n",
    "    print(lang)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy_score(y_test, randomClass) * 100.0))\n",
    "    print(\"F1 Score: %.2f%%\" % (100*f1_score(y_test, y_pred,average=\"weighted\")))\n",
    "    print(\"F1 Score Random: %.2f%%\" % (100*f1_score(y_test, randomClass,average=\"weighted\")))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Repeat same experiment by colapsing overlap and same as one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "Accuracy: 80.31%\n",
      "AccuracyRandom: 44.09%\n",
      "F1 Score: 79.47%\n",
      "F1 Score Random: 46.20%\n",
      "[[81  8]\n",
      " [17 21]]\n",
      "ar\n",
      "Accuracy: 95.43%\n",
      "AccuracyRandom: 48.57%\n",
      "F1 Score: 94.72%\n",
      "F1 Score Random: 58.59%\n",
      "[[159   0]\n",
      " [  8   8]]\n",
      "es\n",
      "Accuracy: 88.31%\n",
      "AccuracyRandom: 63.64%\n",
      "F1 Score: 88.27%\n",
      "F1 Score Random: 64.19%\n",
      "[[44  4]\n",
      " [ 5 24]]\n",
      "fr\n",
      "Accuracy: 89.79%\n",
      "AccuracyRandom: 50.21%\n",
      "F1 Score: 89.39%\n",
      "F1 Score Random: 55.32%\n",
      "[[182   8]\n",
      " [ 16  29]]\n",
      "ja\n",
      "Accuracy: 83.58%\n",
      "AccuracyRandom: 49.25%\n",
      "F1 Score: 84.06%\n",
      "F1 Score Random: 54.05%\n",
      "[[141  20]\n",
      " [ 13  27]]\n",
      "ru\n",
      "Accuracy: 84.91%\n",
      "AccuracyRandom: 53.33%\n",
      "F1 Score: 84.72%\n",
      "F1 Score Random: 55.32%\n",
      "[[182  18]\n",
      " [ 25  60]]\n"
     ]
    }
   ],
   "source": [
    "test_size =.3\n",
    "for lang in langs:\n",
    "    data = allData[allData.lang == lang]\n",
    "    Y = data['binaryRelation']\n",
    "    X = data[['coOccurs','editDistance','isSubSet','tfIdfSimilarity','vectorDistance']]\n",
    "    X2 = poly.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X2, Y, test_size=test_size, random_state=1)\n",
    "    model = RandomForestClassifier(n_estimators=40,random_state=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    randomClass = [choice(['not related','related']) for _ in range(0,len(y_pred))]\n",
    "    print(lang)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    print(\"AccuracyRandom: %.2f%%\" % (accuracy_score(y_test, randomClass) * 100.0))\n",
    "    print(\"F1 Score: %.2f%%\" % (100*f1_score(y_test, y_pred,average=\"weighted\")))\n",
    "    print(\"F1 Score Random: %.2f%%\" % (100*f1_score(y_test, randomClass,average=\"weighted\")))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on K-1 lang, test on the remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "Accuracy: 78.01%\n",
      "Accuracy: 32.15%\n",
      "F1 Score: 75.90%\n",
      "F1 Score Random: 38.32%\n",
      "[[298  11   6]\n",
      " [ 22  18  18]\n",
      " [ 28   8  14]]\n",
      "ar\n",
      "Accuracy: 75.64%\n",
      "Accuracy: 33.28%\n",
      "F1 Score: 82.23%\n",
      "F1 Score Random: 45.92%\n",
      "[[426  56  52]\n",
      " [  1   0   1]\n",
      " [ 13  19  15]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n",
      "Accuracy: 77.56%\n",
      "Accuracy: 31.89%\n",
      "F1 Score: 74.27%\n",
      "F1 Score Random: 34.08%\n",
      "[[166   1   1]\n",
      " [ 31  18  13]\n",
      " [  5   6  13]]\n",
      "fr\n",
      "Accuracy: 86.33%\n",
      "Accuracy: 34.36%\n",
      "F1 Score: 86.12%\n",
      "F1 Score Random: 43.20%\n",
      "[[620  22  15]\n",
      " [ 29  29  25]\n",
      " [ 11   5  27]]\n",
      "ja\n",
      "Accuracy: 58.21%\n",
      "Accuracy: 35.22%\n",
      "F1 Score: 60.60%\n",
      "F1 Score Random: 41.71%\n",
      "[[360  13 142]\n",
      " [ 65   9  25]\n",
      " [ 30   5  21]]\n",
      "ru\n",
      "Accuracy: 77.77%\n",
      "Accuracy: 31.30%\n",
      "F1 Score: 74.59%\n",
      "F1 Score Random: 36.27%\n",
      "[[654   8   6]\n",
      " [ 84  52  18]\n",
      " [ 42  53  32]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "improvement = {'lang':[],'f1Score':[],'accuracy':[],'oneFeatureAcc':[],'oneFeatureF1':[]}\n",
    "test_size =.3\n",
    "for lang in langs:\n",
    "    Y = allData['relation']\n",
    "    X = allData[['coOccurs','editDistance','isSubSet','tfIdfSimilarity','vectorDistance']]\n",
    "    X_train = X[allData.lang != lang]\n",
    "    X_test = X[allData.lang == lang]\n",
    "    y_train = Y[allData.lang != lang]\n",
    "    y_test =  Y[allData.lang == lang]\n",
    "    model = RandomForestClassifier(n_estimators=40,random_state=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    #create Baselines\n",
    "    randomClass = [choice(['not related','overlap','same']) for _ in range(0,len(y_pred))]\n",
    "    randAccuracy = accuracy_score(y_test, randomClass)\n",
    "\n",
    "    modelOneFeature = RandomForestClassifier(n_estimators=40,random_state=1)\n",
    "    modelOneFeature.fit(X_train[['vectorDistance']], y_train)\n",
    "    y_predOneFeature = modelOneFeature.predict(X_test[['vectorDistance']])\n",
    "    oneFeatureAccuracy = accuracy_score(y_test, randomClass)\n",
    "\n",
    "    print(lang)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    print(\"Accuracy: %.2f%%\" % (randAccuracy * 100.0))\n",
    "    print(\"F1 Score: %.2f%%\" % (100*f1_score(y_test, y_pred,average=\"weighted\")))\n",
    "    print(\"F1 Score Random: %.2f%%\" % (100*f1_score(y_test, randomClass,average=\"weighted\")))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    improvement['lang'].append(lang)\n",
    "    improvement['f1Score'].append(accuracy-randAccuracy)\n",
    "    improvement['accuracy'].append(f1_score(y_test, y_pred,average=\"weighted\")-f1_score(y_test, randomClass,average=\"weighted\"))\n",
    "    improvement['oneFeatureAcc'].append(accuracy-oneFeatureAccuracy)\n",
    "    improvement['oneFeatureF1'].append(f1_score(y_test, y_pred,average=\"weighted\")-f1_score(y_test, y_predOneFeature,average=\"weighted\"))\n",
    "improvement = pd.DataFrame(improvement).set_index('lang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "{} &  accuracy &  f1Score \\\\\n",
      "lang &           &          \\\\\n",
      "\\midrule\n",
      "en   &      0.38 &     0.46 \\\\\n",
      "ar   &      0.36 &     0.42 \\\\\n",
      "es   &      0.40 &     0.46 \\\\\n",
      "fr   &      0.43 &     0.52 \\\\\n",
      "ja   &      0.19 &     0.23 \\\\\n",
      "ru   &      0.38 &     0.46 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(improvement.round(2)[['accuracy','f1Score']].to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "{} &  f1Score \\\\\n",
      "lang &          \\\\\n",
      "\\midrule\n",
      "en   &     0.46 \\\\\n",
      "ar   &     0.42 \\\\\n",
      "es   &     0.46 \\\\\n",
      "fr   &     0.52 \\\\\n",
      "ja   &     0.23 \\\\\n",
      "ru   &     0.46 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(improvement.round(2)[['f1Score']].to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "{} &  accuracy &  f1Score &  oneFeatureAcc &  oneFeatureF1 \\\\\n",
      "lang &           &          &                &               \\\\\n",
      "\\midrule\n",
      "en   &       0.4 &      0.5 &            0.5 &           0.0 \\\\\n",
      "ar   &       0.4 &      0.4 &            0.4 &          -0.0 \\\\\n",
      "es   &       0.4 &      0.5 &            0.5 &           0.1 \\\\\n",
      "fr   &       0.4 &      0.5 &            0.5 &           0.0 \\\\\n",
      "ja   &       0.2 &      0.2 &            0.2 &           0.1 \\\\\n",
      "ru   &       0.4 &      0.5 &            0.5 &           0.0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(improvement.round(1).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "Accuracy: 83.45%\n",
      "Accuracy: 32.77%\n",
      "F1 Score: 82.41%\n",
      "F1 Score Random: 52.61%\n",
      "[[296  19]\n",
      " [ 51  57]]\n",
      "ar\n",
      "Accuracy: 79.76%\n",
      "Accuracy: 32.77%\n",
      "F1 Score: 83.74%\n",
      "F1 Score Random: 61.11%\n",
      "[[428 106]\n",
      " [ 12  37]]\n",
      "es\n",
      "Accuracy: 84.65%\n",
      "Accuracy: 32.77%\n",
      "F1 Score: 83.61%\n",
      "F1 Score Random: 48.13%\n",
      "[[164   4]\n",
      " [ 35  51]]\n",
      "fr\n",
      "Accuracy: 90.29%\n",
      "Accuracy: 32.77%\n",
      "F1 Score: 90.23%\n",
      "F1 Score Random: 55.10%\n",
      "[[621  36]\n",
      " [ 40  86]]\n",
      "ja\n",
      "Accuracy: 70.90%\n",
      "Accuracy: 32.77%\n",
      "F1 Score: 72.59%\n",
      "F1 Score Random: 54.03%\n",
      "[[380 135]\n",
      " [ 60  95]]\n",
      "ru\n",
      "Accuracy: 86.09%\n",
      "Accuracy: 32.77%\n",
      "F1 Score: 85.04%\n",
      "F1 Score Random: 51.32%\n",
      "[[653  15]\n",
      " [117 164]]\n"
     ]
    }
   ],
   "source": [
    "# repeat previous but with two classes\n",
    "test_size =.3\n",
    "improvement2 = {'lang':[],'f1Score':[],'accuracy':[]}\n",
    "\n",
    "for lang in langs:\n",
    "    Y = allData['binaryRelation']\n",
    "    X = allData[['coOccurs','editDistance','isSubSet','tfIdfSimilarity','vectorDistance']]\n",
    "    X2 = poly.fit_transform(X)\n",
    "\n",
    "    X_train = X2[allData.lang != lang]\n",
    "    X_test = X2[allData.lang == lang]\n",
    "    y_train = Y[allData.lang != lang]\n",
    "    y_test =  Y[allData.lang == lang]\n",
    "    model = RandomForestClassifier(n_estimators=40,random_state=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    randomClass = [choice(['not related','related']) for _ in range(0,len(y_pred))]\n",
    "\n",
    "    print(lang)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    print(\"Accuracy: %.2f%%\" % (randAccuracy * 100.0))\n",
    "    print(\"F1 Score: %.2f%%\" % (100*f1_score(y_test, y_pred,average=\"weighted\")))\n",
    "    print(\"F1 Score Random: %.2f%%\" % (100*f1_score(y_test, randomClass,average=\"weighted\")))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    improvement2['lang'].append(lang)\n",
    "    improvement2['f1Score'].append(accuracy-randAccuracy)\n",
    "    improvement2['accuracy'].append(f1_score(y_test, y_pred,average=\"weighted\")-f1_score(y_test, randomClass,average=\"weighted\"))\n",
    "improvement2 = pd.DataFrame(improvement2).set_index('lang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "{} &  f1Score2\\_class &  f1Score \\\\\n",
      "lang &                 &          \\\\\n",
      "\\midrule\n",
      "en   &            0.51 &     0.46 \\\\\n",
      "ar   &            0.47 &     0.42 \\\\\n",
      "es   &            0.52 &     0.46 \\\\\n",
      "fr   &            0.58 &     0.52 \\\\\n",
      "ja   &            0.38 &     0.23 \\\\\n",
      "ru   &            0.53 &     0.46 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(improvement2.join(improvement,lsuffix='2_class').round(2)[['f1Score2_class','f1Score']].to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All toghether (not considering languages)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ru\n",
      "Accuracy: 80.44%\n",
      "Accuracy: 34.12%\n",
      "F1 Score: 79.36%\n",
      "F1 Score Random: 41.80%\n",
      "[[816  36  21]\n",
      " [ 63  31  29]\n",
      " [ 34  32  37]]\n"
     ]
    }
   ],
   "source": [
    "    test_size =.3\n",
    "    Y = allData['relation']\n",
    "    X = allData[['coOccurs','editDistance','isSubSet','tfIdfSimilarity','vectorDistance']]\n",
    "    X2 = poly.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X2, Y, test_size=test_size, random_state=1)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=40,random_state=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    randomClass = [choice(['not related','overlap','same']) for _ in range(0,len(y_pred))]\n",
    "    print(lang)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy_score(y_test, randomClass) * 100.0))\n",
    "    print(\"F1 Score: %.2f%%\" % (100*f1_score(y_test, y_pred,average=\"weighted\")))\n",
    "    print(\"F1 Score Random: %.2f%%\" % (100*f1_score(y_test, randomClass,average=\"weighted\")))\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.35%\n",
      "AccuracyRandom: 49.14%\n",
      "F1 Score: 86.03%\n",
      "F1 Score Random: 53.93%\n",
      "[[811  62]\n",
      " [ 88 138]]\n"
     ]
    }
   ],
   "source": [
    "    test_size =.3\n",
    "    Y = allData['binaryRelation']\n",
    "    X = allData[['coOccurs','editDistance','isSubSet','tfIdfSimilarity','vectorDistance']]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=1)\n",
    "    model = RandomForestClassifier(n_estimators=40,random_state=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    randomClass = [choice(['not related','related']) for _ in range(0,len(y_pred))]\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    print(\"AccuracyRandom: %.2f%%\" % (accuracy_score(y_test, randomClass) * 100.0))\n",
    "    print(\"F1 Score: %.2f%%\" % (100*f1_score(y_test, y_pred,average=\"weighted\")))\n",
    "    print(\"F1 Score Random: %.2f%%\" % (100*f1_score(y_test, randomClass,average=\"weighted\")))\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
