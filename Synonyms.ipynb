{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synonym detection\n",
    "This script output a list of candidates for sections 'synonyms' \n",
    "Potential synonyms must:\n",
    "    * Co-occur with similar sections (measured with tfidf metric, threshold fixed in minSimilarity parameter)\n",
    "    * Don't co-ocurr with between them more than a certain treshold (maxCooccur parameter)\n",
    "Additioanlly, other features are added for later evaluatio\n",
    "    * editdistance\n",
    "    * fasttext distance\n",
    "\n",
    "Inputs: \n",
    "    * Sections per article contained in ../gap/multiLanguageFromDumpsSec/sections-articles_lang.json, in format {articleId_1:[sec_a,sec_b...], articleId_2:[sec_x,sec_y], ..., article_n:[sec_i...]}\n",
    "    \n",
    "(The actual values uploaded to gdocs are generated with the .py version in this same folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from collections import Counter\n",
    "import gzip\n",
    "import json\n",
    "import itertools\n",
    "import networkx as nx\n",
    "from functools import reduce\n",
    "from itertools import combinations\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "import numpy as np\n",
    "import editdistance\n",
    "from fastText_multilingual.fasttext import FastVector\n",
    "import re\n",
    "\n",
    "def fasttextDistance(sec1,sec2,vectors):\n",
    "    '''\n",
    "    Take two sections, create a vector for each of them summing all the words\n",
    "    return cosine similarity\n",
    "    '''\n",
    "    sec1 = sec1.lower().split()\n",
    "    sec2 = sec2.lower().split()\n",
    "    sec1Vector  = sum([vectors[word] for word in sec1 if word in vectors])/len(sec1)\n",
    "    sec2Vector  = sum([vectors[word] for word in sec2 if word in vectors])/len(sec2)\n",
    "    distance  = vectors.cosine_similarity(sec1Vector,sec2Vector)\n",
    "    if not isinstance(distance,float): #when at least one of the sections is not the vectorial space, the result is 'nan'\n",
    "        return 0\n",
    "    else:\n",
    "        return vectors.cosine_similarity(sec1Vector,sec2Vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters \n",
    "\n",
    "#langs=['es','en','ar','ja','ru','fr']##define languages\n",
    "langs = ['en','es']\n",
    "p = 0.75 #percentage of sections occurrences to be corevered \n",
    "maxCooccur = 3 #Maximum of coocurrences between pair of sections to be considered synonyms\n",
    "minSimilarity = .6# Miminum cosine similarity to be consider synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and save candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n"
     ]
    }
   ],
   "source": [
    "dfs = {}\n",
    "for lang in langs:\n",
    "    print(lang)\n",
    "    output = []\n",
    "    coOccur = {}\n",
    "    sectionsAll = []\n",
    "    #Load Sections\n",
    "    with open('../gap/multiLanguageFromDumpsSec/sections-articles_%s.json' % lang) as f: \n",
    "        sections = json.load(f)\n",
    "    ##get most frequent sections\n",
    "    for secs in sections.values():\n",
    "        for secName in secs:\n",
    "                cleanSection = re.sub('[=\\]\\[]','',secName).strip()\n",
    "                sectionsAll.append(cleanSection.strip())\n",
    "    sectionsFreq = Counter(sectionsAll)\n",
    "    total = sum(sectionsFreq.values())\n",
    "    acc =0\n",
    "    secsToEval = []\n",
    "    for n,(sec,freq) in enumerate(sectionsFreq.most_common()):\n",
    "        acc+= freq\n",
    "        secsToEval.append(sec)\n",
    "\n",
    "        if acc/total > p: #using sections that cover 80% of total\n",
    "                break\n",
    "    ## Get fasttext vectors for lang\n",
    "    wordVectors = FastVector(vector_file='fastText_multilingual/vectors/wiki.%s.vec' % lang)\n",
    "    ## Count Coocurrences of sections\n",
    "    for page,secs in sections.items():\n",
    "        for sec1,sec2 in combinations(secs,2):\n",
    "                coOccur[sec1] = coOccur.get(sec1,{})\n",
    "                coOccur[sec2] = coOccur.get(sec2,{})\n",
    "                coOccur[sec1][sec2] = coOccur[sec1].get(sec2,0)\n",
    "                coOccur[sec2][sec1] = coOccur[sec2].get(sec1,0)\n",
    "                coOccur[sec1][sec2] += 1\n",
    "                coOccur[sec2][sec1] += 1\n",
    "    \n",
    "    #Compute the IDF, different from working with words, sections names can just occur ones per doc\n",
    "    idf = {}\n",
    "    for sec in coOccur.keys():\n",
    "        idf[sec] = math.log(len(sectionsFreq) / (1 + sectionsFreq[sec]))\n",
    "    #compute TFIDF\n",
    "    tfidf = {}\n",
    "    for sec1,secs in coOccur.items():\n",
    "        if (sec1 in secsToEval):\n",
    "            tfidf[sec1] = {}\n",
    "            for sec2,tf in secs.items():\n",
    "                tfidf[sec1][sec2] = tf * idf[sec2]\n",
    "\n",
    "    #Transform dictionary to sparse matrix\n",
    "    v = DictVectorizer()\n",
    "    tfidfVectors = v.fit_transform(tfidf.values())\n",
    "    tfidfKeys = tfidf.keys()\n",
    "    \n",
    "    #Compute pairwise cosine similariry\n",
    "    S = cosine_similarity(tfidfVectors)\n",
    "    \n",
    "    #Find most similar pairs\n",
    "    np.fill_diagonal(S, -1) #'remove' diagional \n",
    "    tri_upper_diag = np.triu(S, k=0) #given that the matrix is symetric I take just thre upper triangle\n",
    "    mostSimilar = np.where( tri_upper_diag > minSimilarity)\n",
    "    \n",
    "\n",
    "    indexes = {n:k for n,k in enumerate(tfidfKeys)}\n",
    "    for sec1,sec2 in zip(mostSimilar[0],mostSimilar[1]):\n",
    "        if coOccur[indexes[sec2]].get(indexes[sec1],0) <= maxCooccur:\n",
    "            sec1Name = indexes[sec1]\n",
    "            sec2Name = indexes[sec2]\n",
    "            tfIdfsimilarity = tri_upper_diag[sec1][sec2]\n",
    "            editDistance = editdistance.eval(sec1Name, sec2Name)\n",
    "            isSubSet = (sec1Name.lower() in sec2Name.lower()) or (sec2Name.lower() in sec1Name.lower()) \n",
    "            vectorDistance = fasttextDistance(sec1Name,sec2Name,wordVectors)\n",
    "            output.append({'Sec_A':indexes[sec1],'Sec_B':indexes[sec2],\n",
    "                           'coOccurs':coOccur[indexes[sec2]].get(indexes[sec1],0),\n",
    "                           'tfIdfSimilarity':round(tri_upper_diag[sec1][sec2],2),\n",
    "                           'editDistance': editDistance,\n",
    "                           'isSubSet': isSubSet,\n",
    "                           'vectorDistance':vectorDistance,                           \n",
    "                          })\n",
    "    #save results in xls\n",
    "    df = pd.DataFrame(output)\n",
    "    df = df.sort_values(['tfIdfSimilarity','vectorDistance','editDistance','isSubSet'],ascending=False)\n",
    "    print(df)\n",
    "    df.to_excel('%sSynonyms.xls' % lang,index=False)\n",
    "    print(df.corr())\n",
    "    df[lang] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
